# -*- coding: utf-8 -*-
"""
YARA Rule and Hash Generator

This script analyzes one or more files to produce a suite of YARA rules based on
different file characteristics (signatures, entropy, strings, indicators, and
memory-related API calls). It also calculates each file's SHA-256 hash.

The script has two modes:
1. Default: Generate a separate set of rules for each file provided.
   Usage: python yara_generator.py <file1_path> [file2_path] ...

2. Combined Rule: Generate one rule from all provided files.
   Usage: python yara_generator.py --combine <file1_path> <file2_path> ...

Example (Individual):
    python yara_generator.py my_malware.exe another_sample.dll

Example (Combined):
    python yara_generator.py --combine sample1.exe sample2.exe sample3.exe
"""

import sys
import hashlib
import os
import re
import math
from datetime import datetime
import collections

# --- Configuration ---
STRING_LENGTH = 16  # Length of each byte string to extract for the signature rule.
ENTROPY_CHUNK_SIZE = 256 # Size of blocks to calculate entropy over.
STRINGS_PER_FILE_COMBINED = 3 # Number of hex strings to extract from each file for a combined rule.

def calculate_sha256(file_path):
    """
    Calculates the SHA-256 hash of a file. Reads the file in chunks
    to handle large files efficiently.
    """
    sha256_hash = hashlib.sha256()
    try:
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    except IOError as e:
        print(f"Error reading file for hashing: {e}")
        return None

def get_rule_header(rule_name_suffix, file_path, file_hash, file_content):
    """Generates a standard 'rule' and 'meta' section for a YARA rule."""
    file_size = len(file_content)
    file_name = os.path.basename(file_path)
    clean_file_name = re.sub(r'[^a-zA-Z0-9_]', '', os.path.splitext(file_name)[0])
    rule_name = f"{rule_name_suffix}_{clean_file_name}"

    meta_section = [
        f'description = "Autogenerated YARA rule for identifying \'{file_name}\' ({rule_name_suffix})"',
        f'author = "YARA Rule Generator Script"',
        f'date = "{datetime.now().strftime("%Y-%m-%d")}"',
        f'sha256 = "{file_hash}"',
        f'file_size = "{file_size} bytes"'
    ]

    header = f"""rule {rule_name}
{{
    meta:
{chr(10).join(f"        {line}" for line in meta_section)}
"""
    return header

# --- Rule Generation Functions ---

def generate_signature_rule(file_path, file_hash, file_content):
    """
    Generates a YARA rule based on the file's magic number and unique
    byte sequences at varied offsets.
    """
    file_size = len(file_content)
    header = get_rule_header("Signature", file_path, file_hash, file_content)

    # --- Strings Section ---
    strings_section = []
    string_vars = []
    
    # 1. Get Magic Number (first 4 bytes) for the condition
    magic_number = None
    if file_size >= 4:
        magic_number = int.from_bytes(file_content[:4], 'little')

    # 2. Extract unique byte sequences at varied offsets
    offsets = [
        int(file_size * 0.1),
        int(file_size * 0.5),
        max(0, file_size - STRING_LENGTH - int(file_size * 0.1))
    ]
    unique_offsets = sorted(list(set(offsets)))

    for i, offset in enumerate(unique_offsets):
        if offset + STRING_LENGTH <= file_size:
            chunk = file_content[offset:offset + STRING_LENGTH]
            hex_string = chunk.hex()
            var_name = f"$hex_{i+1}"
            strings_section.append(f'{var_name} = {{ {hex_string} }}')
            string_vars.append(var_name)

    # --- Condition Section (ordered for performance) ---
    conditions = []
    if file_size > 0:
        conditions.append(f"filesize == {file_size}")
    if magic_number is not None:
        conditions.append(f"uint32(0) == 0x{magic_number:x}")
    if len(string_vars) > 1:
        conditions.append(f"all of them")
    elif len(string_vars) == 1:
        conditions.append(f"{string_vars[0]}")


    condition_str = " and ".join(conditions) if conditions else "false"

    rule = f"""{header}
    strings:
{chr(10).join(f"        {line}" for line in strings_section) if strings_section else "        // No unique strings found to add."}

    condition:
        {condition_str}
}}"""
    return rule

def calculate_entropy(data):
    """Helper function to calculate Shannon entropy of a byte string."""
    if not data:
        return 0
    entropy = 0
    for x in range(256):
        p_x = float(data.count(x))/len(data)
        if p_x > 0:
            entropy += - p_x * math.log(p_x, 2)
    return entropy

def generate_entropy_rule(file_path, file_hash, file_content):
    """
    Finds a block with high entropy (packed/encrypted data) and generates
    a YARA rule to detect it.
    """
    file_size = len(file_content)
    if file_size < ENTROPY_CHUNK_SIZE:
        return None # File too small for meaningful entropy analysis

    header = get_rule_header("HighEntropy", file_path, file_hash, file_content)
    
    highest_entropy = 0
    best_offset = -1

    # Find the block with the highest entropy
    for i in range(0, file_size - ENTROPY_CHUNK_SIZE, ENTROPY_CHUNK_SIZE):
        chunk = file_content[i:i + ENTROPY_CHUNK_SIZE]
        entropy = calculate_entropy(chunk)
        if entropy > highest_entropy:
            highest_entropy = entropy
            best_offset = i
    
    # Only create a rule if entropy is significantly high (suggests packing/encryption)
    if highest_entropy < 7.5:
        return f"// No high entropy sections found in {os.path.basename(file_path)} (max: {highest_entropy:.2f}). No rule generated."

    condition = f"math.entropy(0x{best_offset:x}, {ENTROPY_CHUNK_SIZE}) > {highest_entropy - 0.1:.2f}"

    rule = f"""{header}
    imports:
        "math"

    condition:
        filesize > {ENTROPY_CHUNK_SIZE} and {condition}
}}"""
    return rule

def generate_indicator_rule(file_path, file_hash, file_content):
    """
    Generates a YARA rule by searching for IoCs like IPs and URLs.
    """
    header = get_rule_header("Indicators", file_path, file_hash, file_content)
    
    # Regex to find printable ASCII strings
    strings = re.findall(b"[ -~]{6,}", file_content)
    decoded_strings = [s.decode('ascii', errors='ignore') for s in strings]

    # Find potential IoCs
    ips = re.findall(r"\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b", "\n".join(decoded_strings))
    urls = re.findall(r"https?://[^\s/$.?#].[^\s]*", "\n".join(decoded_strings))
    
    unique_ips = sorted(list(set(ips)))
    unique_urls = sorted(list(set(urls)))

    if not unique_ips and not unique_urls:
        return f"// No IP/URL indicators found in {os.path.basename(file_path)}. No rule generated."

    strings_section = []
    for i, ip in enumerate(unique_ips):
        strings_section.append(f'$ip_{i+1} = "{ip}" ascii wide')
    for i, url in enumerate(unique_urls):
        strings_section.append(f'$url_{i+1} = "{url}" ascii wide')

    condition = "filesize > 0 and any of them"
    
    rule = f"""{header}
    strings:
{chr(10).join(f"        {line}" for line in strings_section)}

    condition:
        {condition}
}}"""
    return rule

def generate_strings_rule(file_path, file_hash, file_content):
    """
    Generates a YARA rule based on the longest unique printable string.
    """
    header = get_rule_header("SuspiciousStrings", file_path, file_hash, file_content)

    strings = re.findall(b"[ -~]{10,}", file_content)
    if not strings:
        return f"// No significant printable strings found in {os.path.basename(file_path)}. No rule generated."

    # Find the longest string
    longest_string = max(strings, key=len).decode('ascii', errors='ignore').replace("\\", "\\\\").replace('"', '\\"')

    strings_section = [f'$long_str = "{longest_string}" ascii wide']
    condition = "filesize > 0 and $long_str"

    rule = f"""{header}
    strings:
{chr(10).join(f"        {line}" for line in strings_section)}

    condition:
        {condition}
}}"""
    return rule

def generate_memory_api_rule(file_path, file_hash, file_content):
    """
    Generates a YARA rule by searching for common memory-related Windows API calls.
    This is a strong indicator for packed or malicious executables.
    """
    header = get_rule_header("MemoryAPI", file_path, file_hash, file_content)

    # A curated list of suspicious Windows API functions related to memory,
    # process injection, and dynamic library loading.
    SUSPICIOUS_APIS = [
        "VirtualAlloc", "VirtualAllocEx", "VirtualProtect", "VirtualProtectEx",
        "WriteProcessMemory", "CreateRemoteThread", "ReadProcessMemory",
        "HeapAlloc", "CreateProcessA", "CreateProcessW", "ResumeThread",
        "NtMapViewOfSection", "NtUnmapViewOfSection", "LoadLibraryA", "LoadLibraryW",
        "GetProcAddress", "IsDebuggerPresent"
    ]

    found_apis = []
    for api in SUSPICIOUS_APIS:
        # Search for both ASCII and wide-char versions of the string
        if api.encode('ascii') in file_content or api.encode('utf-16le') in file_content:
            found_apis.append(api)

    if not found_apis:
        return f"// No suspicious memory API strings found in {os.path.basename(file_path)}. No rule generated."

    strings_section = []
    for i, api in enumerate(found_apis):
        # Use 'nocase' to be more robust against simple obfuscation
        strings_section.append(f'$api_{i+1} = "{api}" nocase ascii wide')
    
    # Best practice: Check for PE file magic number ("MZ") first for performance.
    condition = "uint16(0) == 0x5a4d and any of them"

    rule = f"""{header}
    strings:
{chr(10).join(f"        {line}" for line in strings_section)}

    condition:
        {condition}
}}"""
    return rule

def generate_combined_rule(file_paths):
    """
    Generates a single YARA rule from a list of files.
    """
    print("\n[*] Generating a combined rule for all provided files...")
    print("=" * 62)

    files_data = []
    for path in file_paths:
        if not os.path.isfile(path):
            print(f"Warning: Skipping invalid file path '{path}'")
            continue
        try:
            with open(path, "rb") as f:
                content = f.read()
            sha256 = calculate_sha256(path)
            files_data.append({"path": path, "hash": sha256, "content": content})
        except IOError as e:
            print(f"Error reading file '{path}': {e}. Skipping.")

    if not files_data:
        print("No valid files to process for combined rule.")
        return

    # --- Header ---
    rule_name = f"Combined_Rule_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    hashes = [f"        sha256_{i+1} = \"{d['hash']}\" // {os.path.basename(d['path'])}" for i, d in enumerate(files_data)]
    
    header = f"""rule {rule_name}
{{
    meta:
        description = "Autogenerated combined YARA rule for a group of related files."
        author = "YARA Rule Generator Script"
        date = "{datetime.now().strftime('%Y-%m-%d')}"
{chr(10).join(hashes)}
"""

    # --- Strings ---
    all_strings = []
    for i, data in enumerate(files_data):
        file_content = data['content']
        file_size = len(file_content)
        offsets = [int(file_size * 0.2), int(file_size * 0.4), int(file_size * 0.6)]
        for j, offset in enumerate(offsets):
             if offset + STRING_LENGTH <= file_size:
                chunk = file_content[offset:offset+STRING_LENGTH]
                hex_string = chunk.hex()
                # Use file and string index for unique variable names
                all_strings.append(f'$f{i+1}_s{j+1} = {{ {hex_string} }}')

    # --- Condition ---
    min_size = min(len(d['content']) for d in files_data)
    max_size = max(len(d['content']) for d in files_data)

    # A more robust condition for a combined rule
    # Require matches from at least half the files to be confident.
    num_files = len(files_data)
    required_matches = math.ceil(num_files / 2)
    
    conditions = [f"filesize > {min_size - 1} and filesize < {max_size + 1}"]
    condition_str = f"({ ' or '.join([f'1 of ($f{i+1}_*)' for i in range(num_files)]) }) >= {required_matches}"
    conditions.append(condition_str)
    
    final_condition = " and ".join(conditions)

    rule = f"""{header}
    strings:
{chr(10).join(f"        {line}" for line in all_strings)}

    condition:
        {final_condition}
}}"""
    print(rule)
    print("-" * 62)

def process_file(file_path):
    """
    Analyzes a single file and prints its hash and generated YARA rules.
    """
    if not os.path.isfile(file_path):
        print(f"\nError: Path '{file_path}' is not a valid file. Skipping.")
        return

    print(f"\n[*] Analyzing file: {file_path}")
    print("=" * 62)

    # Calculate hash and read content
    file_hash = calculate_sha256(file_path)
    if file_hash:
        print("\n" + "="*22 + " File Hash " + "="*22)
        print(f"SHA-256: {file_hash}")
    
    try:
        with open(file_path, "rb") as f:
            file_content = f.read()
            
        if not file_content:
            print("\n[*] File is empty. Cannot generate rules.")
            return

        # Generate and print all rules
        rules = [
            generate_signature_rule(file_path, file_hash, file_content),
            generate_entropy_rule(file_path, file_hash, file_content),
            generate_indicator_rule(file_path, file_hash, file_content),
            generate_strings_rule(file_path, file_hash, file_content),
            generate_memory_api_rule(file_path, file_hash, file_content),
        ]
        
        print("\n" + "="*20 + " Generated YARA Rules " + "="*20)
        for rule in rules:
            if rule:
                print(rule)
                print("-" * 62)

    except IOError as e:
        print(f"Error reading file '{file_path}': {e}")

def main():
    """Main function to drive the script."""
    if len(sys.argv) < 2:
        print("Usage: python yara_generator.py [--combine] <file1_path> [file2_path] ...")
        sys.exit(1)

    args = sys.argv[1:]
    
    if "--combine" in args:
        # Combined rule mode
        combine_flag_index = args.index("--combine")
        # Remove the flag to get only the file paths
        files_to_process = args[:combine_flag_index] + args[combine_flag_index+1:]
        if not files_to_process:
            print("Error: --combine flag used but no file paths were provided.")
            sys.exit(1)
        generate_combined_rule(files_to_process)
    else:
        # Individual rule mode
        files_to_process = args
        for file_path in files_to_process:
            process_file(file_path)
            if len(files_to_process) > 1 and file_path != files_to_process[-1]:
                 print("\n" + "#" * 62)


if __name__ == "__main__":
    main()

