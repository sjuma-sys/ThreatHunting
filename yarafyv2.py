# -*- coding: utf-8 -*-
"""
YARA Rule and Hash Generator

This script analyzes one or more files to produce a suite of YARA rules based on
different file characteristics (signatures, entropy, strings, indicators, and
memory-related API calls). It also calculates each file's SHA-256 hash.

The script has two modes:
1. Default: Generate a separate set of rules for each file provided.
   Usage: python yara_generator.py <file1_path> [file2_path] ...

2. Combined Rule: Generate one rule from all provided files, using the best
   unique identifiers from each to create distinct conditions.
   Usage: python yara_generator.py --combine <file1_path> <file2_path> ...

Example (Individual):
    python yara_generator.py my_malware.exe another_sample.dll

Example (Combined):
    python yara_generator.py --combine sample1.exe sample2.exe sample3.exe
"""

import sys
import hashlib
import os
import re
import math
from datetime import datetime
import collections

# --- Configuration ---
STRING_LENGTH = 16  # Length of each byte string to extract for the signature rule.
ENTROPY_CHUNK_SIZE = 256 # Size of blocks to calculate entropy over.
STRINGS_PER_FILE_COMBINED = 3 # Number of hex strings to extract from each file for a combined rule.

# A curated list of suspicious Windows API functions related to memory,
# process injection, and dynamic library loading.
SUSPICIOUS_APIS = [
    "VirtualAlloc", "VirtualAllocEx", "VirtualProtect", "VirtualProtectEx",
    "WriteProcessMemory", "CreateRemoteThread", "ReadProcessMemory",
    "HeapAlloc", "CreateProcessA", "CreateProcessW", "ResumeThread",
    "NtMapViewOfSection", "NtUnmapViewOfSection", "LoadLibraryA", "LoadLibraryW",
    "GetProcAddress", "IsDebuggerPresent"
]

def calculate_sha256(file_path):
    """
    Calculates the SHA-256 hash of a file. Reads the file in chunks
    to handle large files efficiently.
    """
    sha256_hash = hashlib.sha256()
    try:
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    except IOError as e:
        print(f"Error reading file for hashing: {e}")
        return None

def get_rule_header(rule_name_suffix, file_path, file_hash, file_content):
    """Generates a standard 'rule' and 'meta' section for a YARA rule."""
    file_size = len(file_content)
    file_name = os.path.basename(file_path)
    clean_file_name = re.sub(r'[^a-zA-Z0-9_]', '', os.path.splitext(file_name)[0])
    rule_name = f"{rule_name_suffix}_{clean_file_name}"

    meta_section = [
        f'description = "Autogenerated YARA rule for identifying \'{file_name}\' ({rule_name_suffix})"',
        f'author = "YARA Rule Generator Script"',
        f'date = "{datetime.now().strftime("%Y-%m-%d")}"',
        f'sha256 = "{file_hash}"',
        f'file_size = "{file_size} bytes"'
    ]

    header = f"""rule {rule_name}
{{
    meta:
{chr(10).join(f"        {line}" for line in meta_section)}
"""
    return header

# --- Rule Generation Functions ---

def generate_signature_rule(file_path, file_hash, file_content):
    """
    Generates a YARA rule based on the file's magic number and unique
    byte sequences at varied offsets.
    """
    file_size = len(file_content)
    header = get_rule_header("Signature", file_path, file_hash, file_content)

    # --- Strings Section ---
    strings_section = []
    string_vars = []
    
    # 1. Get Magic Number (first 4 bytes) for the condition
    magic_number = None
    if file_size >= 4:
        magic_number = int.from_bytes(file_content[:4], 'little')

    # 2. Extract unique byte sequences at varied offsets
    offsets = [
        int(file_size * 0.1),
        int(file_size * 0.5),
        max(0, file_size - STRING_LENGTH - int(file_size * 0.1))
    ]
    unique_offsets = sorted(list(set(offsets)))

    for i, offset in enumerate(unique_offsets):
        if offset + STRING_LENGTH <= file_size:
            chunk = file_content[offset:offset + STRING_LENGTH]
            hex_string = chunk.hex()
            var_name = f"$hex_{i+1}"
            strings_section.append(f'{var_name} = {{ {hex_string} }}')
            string_vars.append(var_name)

    # --- Condition Section (ordered for performance) ---
    conditions = []
    if file_size > 0:
        conditions.append(f"filesize == {file_size}")
    if magic_number is not None:
        conditions.append(f"uint32(0) == 0x{magic_number:x}")
    if len(string_vars) > 1:
        conditions.append(f"all of them")
    elif len(string_vars) == 1:
        conditions.append(f"{string_vars[0]}")


    condition_str = " and ".join(conditions) if conditions else "false"

    rule = f"""{header}
    strings:
{chr(10).join(f"        {line}" for line in strings_section) if strings_section else "        // No unique strings found to add."}

    condition:
        {condition_str}
}}"""
    return rule

def calculate_entropy(data):
    """Helper function to calculate Shannon entropy of a byte string."""
    if not data:
        return 0
    entropy = 0
    for x in range(256):
        p_x = float(data.count(x))/len(data)
        if p_x > 0:
            entropy += - p_x * math.log(p_x, 2)
    return entropy

def generate_entropy_rule(file_path, file_hash, file_content):
    """
    Finds a block with high entropy (packed/encrypted data) and generates
    a YARA rule to detect it.
    """
    file_size = len(file_content)
    if file_size < ENTROPY_CHUNK_SIZE:
        return None # File too small for meaningful entropy analysis

    header = get_rule_header("HighEntropy", file_path, file_hash, file_content)
    
    highest_entropy, best_offset = find_highest_entropy_chunk(file_content)
    
    # Only create a rule if entropy is significantly high (suggests packing/encryption)
    if highest_entropy < 7.5:
        return f"// No high entropy sections found in {os.path.basename(file_path)} (max: {highest_entropy:.2f}). No rule generated."

    condition = f"math.entropy(0x{best_offset:x}, {ENTROPY_CHUNK_SIZE}) > {highest_entropy - 0.1:.2f}"

    rule = f"""{header}
    imports:
        "math"

    condition:
        filesize > {ENTROPY_CHUNK_SIZE} and {condition}
}}"""
    return rule

def generate_indicator_rule(file_path, file_hash, file_content):
    """
    Generates a YARA rule by searching for IoCs like IPs and URLs.
    """
    header = get_rule_header("Indicators", file_path, file_hash, file_content)
    
    unique_ips, unique_urls = find_iocs(file_content)

    if not unique_ips and not unique_urls:
        return f"// No IP/URL indicators found in {os.path.basename(file_path)}. No rule generated."

    strings_section = []
    for i, ip in enumerate(unique_ips):
        strings_section.append(f'$ip_{i+1} = "{ip}" ascii wide')
    for i, url in enumerate(unique_urls):
        strings_section.append(f'$url_{i+1} = "{url}" ascii wide')

    condition = "filesize > 0 and any of them"
    
    rule = f"""{header}
    strings:
{chr(10).join(f"        {line}" for line in strings_section)}

    condition:
        {condition}
}}"""
    return rule

def generate_strings_rule(file_path, file_hash, file_content):
    """
    Generates a YARA rule based on the longest unique printable string.
    """
    header = get_rule_header("SuspiciousStrings", file_path, file_hash, file_content)

    longest_string = find_longest_string(file_content)
    if not longest_string:
        return f"// No significant printable strings found in {os.path.basename(file_path)}. No rule generated."

    strings_section = [f'$long_str = "{longest_string}" ascii wide']
    condition = "filesize > 0 and $long_str"

    rule = f"""{header}
    strings:
{chr(10).join(f"        {line}" for line in strings_section)}

    condition:
        {condition}
}}"""
    return rule

def generate_memory_api_rule(file_path, file_hash, file_content):
    """
    Generates a YARA rule by searching for common memory-related Windows API calls.
    This is a strong indicator for packed or malicious executables.
    """
    header = get_rule_header("MemoryAPI", file_path, file_hash, file_content)
    
    found_apis = find_suspicious_apis(file_content)

    if not found_apis:
        return f"// No suspicious memory API strings found in {os.path.basename(file_path)}. No rule generated."

    strings_section = []
    for i, api in enumerate(found_apis):
        # Use 'nocase' to be more robust against simple obfuscation
        strings_section.append(f'$api_{i+1} = "{api}" nocase ascii wide')
    
    # Best practice: Check for PE file magic number ("MZ") first for performance.
    condition = "uint16(0) == 0x5a4d and any of them"

    rule = f"""{header}
    strings:
{chr(10).join(f"        {line}" for line in strings_section)}

    condition:
        {condition}
}}"""
    return rule

# --- Helper functions for combined rule generation ---

def find_suspicious_apis(file_content):
    found_apis = []
    for api in SUSPICIOUS_APIS:
        if api.encode('ascii') in file_content or api.encode('utf-16le') in file_content:
            found_apis.append(api)
    return found_apis

def find_highest_entropy_chunk(file_content):
    file_size = len(file_content)
    highest_entropy = 0
    best_offset = -1
    if file_size >= ENTROPY_CHUNK_SIZE:
        for i in range(0, file_size - ENTROPY_CHUNK_SIZE, ENTROPY_CHUNK_SIZE):
            chunk = file_content[i:i + ENTROPY_CHUNK_SIZE]
            entropy = calculate_entropy(chunk)
            if entropy > highest_entropy:
                highest_entropy = entropy
                best_offset = i
    return highest_entropy, best_offset

def find_iocs(file_content):
    strings = re.findall(b"[ -~]{6,}", file_content)
    decoded_strings = [s.decode('ascii', errors='ignore') for s in strings]
    ips = re.findall(r"\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b", "\n".join(decoded_strings))
    urls = re.findall(r"https?://[^\s/$.?#].[^\s]*", "\n".join(decoded_strings))
    return sorted(list(set(ips))), sorted(list(set(urls)))

def find_longest_string(file_content):
    strings = re.findall(b"[ -~]{10,}", file_content)
    if not strings:
        return None
    return max(strings, key=len).decode('ascii', errors='ignore').replace("\\", "\\\\").replace('"', '\\"')

def generate_combined_rule(file_paths):
    """
    Generates a single, intelligent YARA rule from a list of related files.
    """
    print("\n[*] Generating a combined rule for all provided files...")
    print("=" * 62)

    files_data = []
    for path in file_paths:
        if not os.path.isfile(path):
            print(f"Warning: Skipping invalid file path '{path}'")
            continue
        try:
            with open(path, "rb") as f:
                content = f.read()
            sha256 = calculate_sha256(path)
            files_data.append({"path": path, "hash": sha256, "content": content})
        except IOError as e:
            print(f"Error reading file '{path}': {e}. Skipping.")

    if not files_data:
        print("No valid files to process for combined rule.")
        return

    # --- Header ---
    rule_name = f"Combined_Rule_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    hashes = [f"        sha256_{i+1} = \"{d['hash']}\" // {os.path.basename(d['path'])}" for i, d in enumerate(files_data)]
    
    header = f"""rule {rule_name}
{{
    meta:
        description = "Autogenerated combined YARA rule for a group of related files."
        author = "YARA Rule Generator Script"
        date = "{datetime.now().strftime('%Y-%m-%d')}"
{chr(10).join(hashes)}
"""

    # --- Strings & Conditions ---
    final_strings_section = []
    final_condition_clauses = []
    needs_math_module = False

    for i, data in enumerate(files_data):
        content = data['content']
        prefix = f"f{i+1}" # e.g., f1, f2, ...

        # --- Logic to find the best, unique identifiers for this file ---
        file_strings = []
        file_condition = None

        # Priority 1: Suspicious Memory APIs (strongest indicator for PE malware)
        found_apis = find_suspicious_apis(content)
        if found_apis:
            api_string_vars = []
            for j, api in enumerate(found_apis):
                var_name = f"${prefix}_api_{j+1}"
                file_strings.append(f'{var_name} = "{api}" nocase ascii wide')
                api_string_vars.append(var_name)
            file_condition = f"(uint16(0) == 0x5a4d and 1 of ({', '.join(api_string_vars)}))"
        
        # Priority 2: High Entropy
        if not file_condition:
            entropy, offset = find_highest_entropy_chunk(content)
            if entropy >= 7.5:
                needs_math_module = True
                file_condition = f"(math.entropy(0x{offset:x}, {ENTROPY_CHUNK_SIZE}) > {entropy - 0.1:.2f})"

        # Priority 3: IoCs (IPs/URLs)
        if not file_condition:
            ips, urls = find_iocs(content)
            if ips or urls:
                ioc_string_vars = []
                for j, ip in enumerate(ips):
                    var_name = f"${prefix}_ip_{j+1}"
                    file_strings.append(f'{var_name} = "{ip}" ascii wide')
                    ioc_string_vars.append(var_name)
                for j, url in enumerate(urls):
                    var_name = f"${prefix}_url_{j+1}"
                    file_strings.append(f'{var_name} = "{url}" ascii wide')
                    ioc_string_vars.append(var_name)
                file_condition = f"(1 of ({', '.join(ioc_string_vars)}))"

        # Priority 4: Fallback to Hex + Longest String
        if not file_condition:
            hex_str_var = None
            text_str_var = None
            
            # Hex string from middle
            offset = int(len(content) * 0.5)
            if offset + STRING_LENGTH <= len(content):
                chunk = content[offset:offset+STRING_LENGTH]
                hex_str_var = f"${prefix}_hex"
                file_strings.append(f'{hex_str_var} = {{ {chunk.hex()} }}')
            
            # Longest string
            longest_str = find_longest_string(content)
            if longest_str:
                text_str_var = f"${prefix}_str"
                file_strings.append(f'{text_str_var} = "{longest_str}" ascii wide')
            
            temp_conds = [v for v in [hex_str_var, text_str_var] if v]
            if temp_conds:
                file_condition = f"({' and '.join(temp_conds)})"

        if file_condition:
            final_strings_section.extend([f"        // --- Indicators for {os.path.basename(data['path'])} ---"] + [f"        {s}" for s in file_strings])
            final_condition_clauses.append(file_condition)

    # --- Assemble Final Rule ---
    min_size = min(len(d['content']) for d in files_data)
    max_size = max(len(d['content']) for d in files_data)

    imports_section = '    imports:\n        "math"\n' if needs_math_module else ""
    
    condition_section = [f"filesize > {min_size - 1} and filesize < {max_size + 1}"]
    if final_condition_clauses:
        condition_section.append(f"(\n            {chr(10).join(f'{c} or' for c in final_condition_clauses[:-1])}\n            {final_condition_clauses[-1]}\n        )")
    
    final_condition = " and ".join(condition_section)

    rule = f"""{header}
{imports_section}
    strings:
{chr(10).join(final_strings_section)}

    condition:
        {final_condition}
}}"""
    print(rule)
    print("-" * 62)


def process_file(file_path):
    """
    Analyzes a single file and prints its hash and generated YARA rules.
    """
    if not os.path.isfile(file_path):
        print(f"\nError: Path '{file_path}' is not a valid file. Skipping.")
        return

    print(f"\n[*] Analyzing file: {file_path}")
    print("=" * 62)

    # Calculate hash and read content
    file_hash = calculate_sha256(file_path)
    if file_hash:
        print("\n" + "="*22 + " File Hash " + "="*22)
        print(f"SHA-256: {file_hash}")
    
    try:
        with open(file_path, "rb") as f:
            file_content = f.read()
            
        if not file_content:
            print("\n[*] File is empty. Cannot generate rules.")
            return

        # Generate and print all rules
        rules = [
            generate_signature_rule(file_path, file_hash, file_content),
            generate_entropy_rule(file_path, file_hash, file_content),
            generate_indicator_rule(file_path, file_hash, file_content),
            generate_strings_rule(file_path, file_hash, file_content),
            generate_memory_api_rule(file_path, file_hash, file_content),
        ]
        
        print("\n" + "="*20 + " Generated YARA Rules " + "="*20)
        for rule in rules:
            if rule:
                print(rule)
                print("-" * 62)

    except IOError as e:
        print(f"Error reading file '{file_path}': {e}")

def main():
    """Main function to drive the script."""
    if len(sys.argv) < 2:
        print("Usage: python yara_generator.py [--combine] <file1_path> [file2_path] ...")
        sys.exit(1)

    args = sys.argv[1:]
    
    if "--combine" in args:
        # Combined rule mode
        combine_flag_index = args.index("--combine")
        # Remove the flag to get only the file paths
        files_to_process = args[:combine_flag_index] + args[combine_flag_index+1:]
        if not files_to_process:
            print("Error: --combine flag used but no file paths were provided.")
            sys.exit(1)
        generate_combined_rule(files_to_process)
    else:
        # Individual rule mode
        files_to_process = args
        for file_path in files_to_process:
            process_file(file_path)
            if len(files_to_process) > 1 and file_path != files_to_process[-1]:
                 print("\n" + "#" * 62)


if __name__ == "__main__":
    main()

